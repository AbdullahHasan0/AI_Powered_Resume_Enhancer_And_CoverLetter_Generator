{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahHasan0/AI_Powered_Resume_Enhancer_And_CoverLetter_Generator/blob/main/Resume_Builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Enhancer\n",
        "\n",
        "Important: OpenAI API Key Required\n",
        "\n",
        "This project uses OpenAI models for analyzing resumes and generating personalized cover letters.  \n",
        "When running on **Google Colab**, you need to provide your OpenAI API key."
      ],
      "metadata": {
        "id": "CkhdhMnbyzXZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCHff7vgggj2"
      },
      "source": [
        "# Install necessary libraries if running for the first time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6Da5T_Dggj5",
        "outputId": "f508ef50-a2ae-4125-ac45-f2892e91964c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (7.34.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "%pip install openai ipython pydantic PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqxFDVsaggj8",
        "outputId": "39dacb32-c31f-4526-d684-0b3317dcd893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Let's install and import Pydantic\n",
        "# In Pydantic, BaseModel is the core class that you use to create data models.\n",
        "# BaseModel is like a blueprint for structured data. It defines the fields, their types, and automatically gives you data validation and type conversion capabilities\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Importing other necessary libraries\n",
        "import os\n",
        "from openai import OpenAI  # Make sure you have the latest openai package (pip install --upgrade openai)\n",
        "import json\n",
        "\n",
        "from typing import List, Dict, Union, Optional, Any\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "from google.colab import userdata\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gL0pPKEDggj_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Fetch API keys from environment variables\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "# Configure the APIs\n",
        "openai_client = OpenAI(api_key = openai_api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-oZExPkggkA"
      },
      "source": [
        "# DEFINING THE LLM INPUTS INCLUDING RESUME AND TARGET JOB DESCRIPTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4VGpan7ggkA"
      },
      "source": [
        "- We need to get the user's resume and the target job description.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "P5dY9iOaggkB"
      },
      "outputs": [],
      "source": [
        "# Helper function to display markdown nicely\n",
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown.\"\"\"\n",
        "    display(Markdown(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lQJp7GWHggkB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "\n",
        "def pick_and_extract_pdf(save_to=\"store\"):\n",
        "    \"\"\"\n",
        "    Opens a file picker to upload a single PDF, extracts its text,\n",
        "    and saves it into a .txt file.\n",
        "\n",
        "    Args:\n",
        "        save_to (str): Folder to save extracted .txt file (default: 'store').\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text\n",
        "    \"\"\"\n",
        "    # File picker\n",
        "    uploaded = files.upload()\n",
        "    pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Make sure output folder exists\n",
        "    if not os.path.exists(save_to):\n",
        "        os.makedirs(save_to)\n",
        "\n",
        "    # Read PDF\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "\n",
        "    # Save text into a .txt file\n",
        "    filename = os.path.splitext(os.path.basename(pdf_path))[0] + \".txt\"\n",
        "    txt_path = os.path.join(save_to, filename)\n",
        "\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "    print(f\"✅ Extracted and saved text to: {txt_path}\")\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_file_path: str) -> str:\n",
        "    text = \"\"\n",
        "    with open(pdf_file_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "6FQofICUwOgP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = pick_and_extract_pdf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "w_dj5imYiHAz",
        "outputId": "17b8079a-8eba-4553-9ad9-dd1d784fbae4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e3fc5e4-8727-40b4-8dac-0887b403b63e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e3fc5e4-8727-40b4-8dac-0887b403b63e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Abdullah_Hasan_AIML_Intern _RESUME.pdf to Abdullah_Hasan_AIML_Intern _RESUME.pdf\n",
            "✅ Extracted and saved text to: store/Abdullah_Hasan_AIML_Intern _RESUME.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "iVOiArnBggkE"
      },
      "outputs": [],
      "source": [
        "# Let's define a sample job description text\n",
        "job_description_text = \"\"\"\n",
        "We are looking for a motivated and enthusiastic AI/ML Intern to join our team and work on cutting-edge projects. This is an excellent opportunity to apply your skills in machine learning, data science, and programming to solve real-world challenges.\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "Work with the team to design and improve machine learning models for specific projects.\n",
        "Clean and prepare data for analysis and model training.\n",
        "Explore and test new machine learning techniques to make models more accurate and efficient.\n",
        "Help deploy and test models in environments similar to production.\n",
        "Collaborate with software engineers to integrate models into existing systems.\n",
        "Stay updated on new AI and machine learning tools and trends\n",
        " Keep detailed records of processes, findings, models, and performance for future reference.\n",
        "\n",
        "\n",
        "Requirements:\n",
        "\n",
        "A degree in Computer Science, Data Science, or a related field is required.\n",
        "Good understanding of machine learning algorithms like regression, classification, and clustering.\n",
        "Proficient in Python and libraries\n",
        "Familiar with cloud platforms like AWS, Google Cloud, or Azure.\n",
        "Knowledge of NLP or computer vision is a plus.\n",
        "Comfortable with version control systems, particularly Git.\n",
        "\n",
        "\n",
        "What We Offer:\n",
        "\n",
        "A supportive environment with mentorship opportunities to grow your skills.\n",
        "Opportunities to work on impactful projects and contribute to innovative products.\n",
        "Access to learning resources and professional development.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vYV-WT9zggkF",
        "outputId": "4401fc65-15c4-4701-e58c-d8fcdefeae87"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**--- Original Resume ---**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Syed Abdullah Hasan  \nAI Engineer   \nKarachi, Pakistan | Ph: +923228220707 | abdullahhasan1045@gmail.com | LinkedIn: Abdullah Hasan  | \nGithub: Abdullah Hasan  \n  \nSummary  \nAspiring AI Engineer with a solid foundation in machine learning, deep learning, computer vision, and \nnatural language processing. Proficient in Python, TensorFlow, PyTorch, LangChain, and modern AI \nframeworks. Hands -on experience in developing and deployi ng end -to-end AI solutions including CNN -\nbased medical diagnostics, LLM -powered QA systems, and NLP sentiment analysis. Fast learner eager to \ncontribute to impactful projects and grow under mentorship in a collaborative environment.\n  \nProfessional Experience   \nData Science Fellow – Bytewise Limited                                                                                                       \nJune 2024 – Sep 2024   \n• Built ML models (including CNNs) for real -world data challenges; contributed to data cleaning, \nscaling, and encoding pipelines.  \n• Partnered with teams to create innovative solutions for complex real -world challenges.  \n• Committed to advancing machine learning skills through practical experience and ongoing \neducation.  \n• Fellow of the Month – June’24, Bytewise Limited   \n  \nAcademic Qualification   \nUniversity Of Karachi - UBIT                                                                                                      2021 – 2024 \n• Bachelors of Science in Computer Science  \n  \nCourse Certifications:  \n• Certified Associate Data Scientist by DataCamp  \n• Data Manipulation with pandas by DataCamp  \n• Exploratory Data Analysis in Python  \n• Machine Learning from Udemy  \n• Data Science by Plus W 株式会社  \n  \nSkills:  \nLanguages & Libraries : Python, NumPy, Pandas, Scikit -learn, TensorFlow, PyTorch, LangChain  \nML & AI Development : Regression, Classification, Clustering, Transfer Learning, CNNs, Attention \nMechanisms,LLMs  \nData Handling : Data cleaning, feature engineering, SQL, NoSQL  \nVisualization : Power BI, Matplotlib, Seaborn  \nAI Deployment : Flask, Streamlit, Gradio  \nSoft Skills : Fast learner, proactive team player, growth -oriented mindset  \n  \nProjects:  \nBrain Hemorrhage Detection Using CNN (EfficientNetB2)  \nFinal Year Project  \n• Developed an end -to-end deep learning pipeline using EfficientNetB2  for multiclass classification \nof 5 brain hemorrhage types.  \n• Leveraged the RSNA_19  dataset and achieved 90%+ validation accuracy  through advanced data \naugmentation and transfer learning.  \n• Preprocessed medical DICOM images using windowing techniques and merged multi -channel \ninputs for model readiness.  \n \n  Student Performance Prediction System  \n• Built a regression model to predict students' math scores using demographic and academic \nfeatures.  \n• Designed a modular Train -Predict pipeline  for scalability, using one-hot encoding  and feature \nstandardization . \n• Achieved an R² score of 88.76  using an SGD Regressor  and deployed a lightweight Flask API  for \nreal-time predictions.  \n \nSentiment Analysis for Amazon Product Reviews  \n• Created a sentiment classifier using a fine-tuned DistilRoBERTa  model, reaching 94.8% \naccuracy . \n• Preprocessed textual data using NLP techniques  (tokenization, lemmatization, stopword removal).  \n• Integrated a FalconsAI summarization model  and built a Gradio UI  for real -time sentiment \nanalysis and review summarization.  \n \nLLM-Powered Contextual Question Answering System  \n• Developed a document -agnostic QA assistant  using LangChain , Ollama , and LLaMA3  with \nRAG architecture . \n• Implemented vector -based context retrieval, dynamic prompt templates, and fallback handling for \nunmatched queries.  \n• Enabled users to upload unstructured documents and get accurate answers based on context.  \n \nReal -Time Traffic Sign Detection with YOLOv8  \n• Built a computer vision system using YOLOv8  for real -time detection and classification of traffic \nsigns.  \n• Trained the model on a custom -labeled dataset, achieving high mAP scores.  \n• Deployed with Gradio on Hugging Face Spaces  for real -time browser inference.  \n \nContent -Based Anime Recommendation System  \n• Developed a recommendation engine using TF-IDF vectorization  and cosine similarity  to \nsuggest similar anime titles.  \n• Cleaned and vectorized textual metadata (genres, synopsis) for better recommendation quality.  \n• Packaged into an interactive application for user input and personalized suggestions.  \n \n "
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's display the original resume\n",
        "print_markdown(\"**--- Original Resume ---**\")\n",
        "print_markdown(resume_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "_cTZmZzYggkG",
        "outputId": "88470400-cb54-4d8c-cbf1-33e3716273d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**--- Target Job Description ---**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nWe are looking for a motivated and enthusiastic AI/ML Intern to join our team and work on cutting-edge projects. This is an excellent opportunity to apply your skills in machine learning, data science, and programming to solve real-world challenges.\n\nKey Responsibilities:\n\nWork with the team to design and improve machine learning models for specific projects.\nClean and prepare data for analysis and model training.\nExplore and test new machine learning techniques to make models more accurate and efficient.\nHelp deploy and test models in environments similar to production.\nCollaborate with software engineers to integrate models into existing systems.\nStay updated on new AI and machine learning tools and trends\n Keep detailed records of processes, findings, models, and performance for future reference.\n\n\nRequirements:\n\nA degree in Computer Science, Data Science, or a related field is required.\nGood understanding of machine learning algorithms like regression, classification, and clustering.\nProficient in Python and libraries\nFamiliar with cloud platforms like AWS, Google Cloud, or Azure.\nKnowledge of NLP or computer vision is a plus.\nComfortable with version control systems, particularly Git.\n\n\nWhat We Offer:\n\nA supportive environment with mentorship opportunities to grow your skills.\nOpportunities to work on impactful projects and contribute to innovative products.\nAccess to learning resources and professional development.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's display the target job desciption\n",
        "print_markdown(\"\\n**--- Target Job Description ---**\")\n",
        "print_markdown(job_description_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnQkn5t0ggkH"
      },
      "source": [
        "# ENHANCING THE RESUME WITH OPENAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpkzp3GDggkH"
      },
      "source": [
        "Now that we have our resume and job description, let's use the OpenAI API to improve the resume to better match the job requirements. We'll make a call to the text generation API and ask it to enhance our resume.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4tan1sfsggkI"
      },
      "outputs": [],
      "source": [
        "def openai_generate(prompt: str,\n",
        "                    model: str = \"gpt-4o-mini\",\n",
        "                    temperature: float = 0.7,\n",
        "                    max_tokens: int = 1500,\n",
        "                    response_format: Optional[dict] = None) -> str | dict:\n",
        "    \"\"\"\n",
        "    Generate text using OpenAI API\n",
        "\n",
        "    This function sends a prompt to OpenAI's API and returns the generated response.\n",
        "    It supports both standard text generation and structured parsing with response_format.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt to send to the model, i.e.: your instructions for the AI\n",
        "        model (str): The OpenAI model to use (default: \"gpt-4o-mini\")\n",
        "        temperature (float): Controls randomness, where lower values make output more deterministic\n",
        "        max_tokens (int): Maximum number of tokens to generate, which limits the response length\n",
        "        response_format (dict): Optional format specification\n",
        "        In simple terms, response_format is optional. If the user gives me a dictionary, cool!\n",
        "        If they don't give me anything, just assume it's None and keep going.\"\n",
        "\n",
        "    Returns:\n",
        "        str or dict: The generated text or parsed structured data, depending on response_format\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Standard text generation without a specific response format\n",
        "        if not response_format:\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model = model,\n",
        "                messages = [\n",
        "                    {\"role\": \"system\",\n",
        "                     \"content\": \"You are a helpful assistant specializing in resume writing and career advice.\",\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature = temperature,\n",
        "                max_tokens = max_tokens)\n",
        "\n",
        "            # Extract just the text content from the response\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        # Structured response generation (e.g., JSON format)\n",
        "        else:\n",
        "            completion = openai_client.beta.chat.completions.parse(\n",
        "                model = model,  # Make sure to use a model that supports parse\n",
        "                messages = [\n",
        "                    # Same system and user messages as above\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a helpful assistant specializing in resume writing and career advice.\",\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                temperature = temperature,\n",
        "                response_format = response_format)\n",
        "\n",
        "            # Return the parsed structured output\n",
        "            return completion.choices[0].message.parsed\n",
        "\n",
        "    except Exception as e:\n",
        "        # Error handling to prevent crashes\n",
        "        return f\"Error generating text: {e}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "79GBJkTDggkJ"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Context:\n",
        "You are a professional resume writer helping a candidate tailor their resume for a specific job opportunity. The resume and job description are provided below.\n",
        "\n",
        "Instruction:\n",
        "Enhance the resume to make it more impactful. Focus on:\n",
        "- Highlighting relevant skills and achievements.\n",
        "- Using strong action verbs and quantifiable results where possible.\n",
        "- Rewriting vague or generic bullet points to be specific and results-driven.\n",
        "- Emphasizing experience and skills most relevant to the job description.\n",
        "- Reorganizing sections if necessary to better match the job requirements.\n",
        "\n",
        "Resume:\n",
        "{resume_text}\n",
        "\n",
        "Output:\n",
        "Provide a revised and improved version of the resume that is well-formatted. Only return the updated resume.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IfHDGCJ1ggkJ",
        "outputId": "4ff92208-6cff-475e-e69b-ffc8ada5ef32"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### OpenAI Response:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```plaintext\nSyed Abdullah Hasan  \nAI Engineer   \nKarachi, Pakistan | Ph: +923228220707 | abdullahhasan1045@gmail.com | LinkedIn: Abdullah Hasan | Github: Abdullah Hasan  \n\n---\n\n**Summary**  \nResults-driven AI Engineer with a strong foundation in machine learning, deep learning, computer vision, and natural language processing. Proficient in Python, TensorFlow, PyTorch, and LangChain, with hands-on experience delivering impactful end-to-end AI solutions, including CNN-based medical diagnostics and LLM-powered QA systems. A fast learner with a growth-oriented mindset, eager to contribute to innovative projects in a collaborative environment.\n\n---\n\n**Professional Experience**  \n\n**Data Science Fellow**  \nBytewise Limited, Karachi, Pakistan  \nJune 2024 – September 2024  \n- Developed and optimized machine learning models, including CNNs, addressing real-world data challenges, resulting in improved model accuracy and efficiency.  \n- Collaborated with cross-functional teams to create innovative solutions for complex data problems, enhancing project delivery timelines by 20%.  \n- Recognized as Fellow of the Month (June 2024) for exemplary contributions and commitment to advancing machine learning skills through practical experience.  \n\n---\n\n**Academic Qualification**  \n**Bachelor of Science in Computer Science**  \nUniversity of Karachi - UBIT  \n2021 – 2024  \n\n---\n\n**Certifications**  \n- Certified Associate Data Scientist, DataCamp  \n- Data Manipulation with pandas, DataCamp  \n- Exploratory Data Analysis in Python, DataCamp  \n- Machine Learning, Udemy  \n- Data Science, Plus W 株式会社  \n\n---\n\n**Skills**  \n- **Programming Languages:** Python  \n- **Libraries & Frameworks:** NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch, LangChain  \n- **Machine Learning Techniques:** Regression, Classification, Clustering, Transfer Learning, CNNs, Attention Mechanisms, LLMs  \n- **Data Handling:** Data cleaning, Feature engineering, SQL, NoSQL  \n- **Data Visualization:** Power BI, Matplotlib, Seaborn  \n- **AI Deployment:** Flask, Streamlit, Gradio  \n- **Soft Skills:** Proactive team player, Fast learner, Growth-oriented mindset  \n\n---\n\n**Projects**  \n\n**Brain Hemorrhage Detection Using CNN (EfficientNetB2)**  \n- Engineered an end-to-end deep learning pipeline utilizing EfficientNetB2 for multiclass classification of five brain hemorrhage types.  \n- Achieved over 90% validation accuracy by employing advanced data augmentation and transfer learning techniques on the RSNA_19 dataset.  \n- Enhanced model readiness by preprocessing medical DICOM images using advanced windowing techniques and integrating multi-channel inputs.  \n\n**Student Performance Prediction System**  \n- Designed a regression model to predict students' math scores leveraging demographic and academic features, achieving an R² score of 88.76 with SGD Regressor.  \n- Developed a modular Train-Predict pipeline for scalability, implementing one-hot encoding and feature standardization, and deployed a lightweight Flask API for real-time predictions.  \n\n**Sentiment Analysis for Amazon Product Reviews**  \n- Constructed a sentiment classifier using a fine-tuned DistilRoBERTa model, achieving an accuracy of 94.8%.  \n- Employed NLP techniques to preprocess textual data, including tokenization and lemmatization, leading to significant performance improvement.  \n- Integrated a summarization model and developed a Gradio UI for real-time sentiment analysis and review summarization.  \n\n**LLM-Powered Contextual Question Answering System**  \n- Developed a document-agnostic QA assistant using LangChain, Ollama, and LLaMA3 with RAG architecture, enhancing user interaction and experience.  \n- Implemented vector-based context retrieval and dynamic prompt templates, providing accurate answers based on user-uploaded unstructured documents.  \n\n**Real-Time Traffic Sign Detection with YOLOv8**  \n- Created a computer vision system utilizing YOLOv8 for real-time detection and classification of traffic signs, achieving high mAP scores on a custom-labeled dataset.  \n- Successfully deployed the model using Gradio on Hugging Face Spaces, enabling real-time browser inference capabilities.  \n\n**Content-Based Anime Recommendation System**  \n- Developed a recommendation engine utilizing TF-IDF vectorization and cosine similarity to suggest similar anime titles, enhancing user engagement.  \n- Processed and vectorized textual metadata for improved recommendation quality, packaging it into an interactive application for user input and personalized suggestions.  \n\n---\n```\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get response from OpenAI API\n",
        "openai_output = openai_generate(prompt, temperature = 0.7)\n",
        "\n",
        "# Display the results\n",
        "print_markdown(\"#### OpenAI Response:\")\n",
        "print_markdown(openai_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJBaSHBcggkL"
      },
      "source": [
        "# PERFORMING A GAP ANALYSIS BETWEEN RESUME & JOB DESCRIPTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuyy3H4CggkL"
      },
      "source": [
        "Now, let's use an LLM to analyze the resume and job description. We want the AI to identify:\n",
        "1.  Key skills/requirements mentioned in the Job Description.\n",
        "2.  Relevant skills/experience present in the Resume.\n",
        "3.  Crucially, the mismatches or gaps – what the job asks for that the resume doesn't highlight well.\n",
        "\n",
        "We'll use OpenAI for this analysis step. We need to craft a clear prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "OzfWEgvQggkN"
      },
      "outputs": [],
      "source": [
        "# Prompt to analyze the resume against the job description\n",
        "\n",
        "def analyze_resume_against_job_description(job_description_text: str, resume_text: str, model: str = \"openai\") -> str:\n",
        "    \"\"\"\n",
        "    Analyze the resume against the job description and return a structured comparison.\n",
        "\n",
        "    Args:\n",
        "        job_description_text (str): The job description text.\n",
        "        resume_text (str): The candidate's resume text.\n",
        "        model (str): The model to use for analysis (\"openai\" or \"gemini\").\n",
        "\n",
        "    Returns:\n",
        "        str: A clear, structured comparison of the resume and job description.\n",
        "    \"\"\"\n",
        "    # This prompt instructs the AI to act as a career advisor and analyze how well the resume matches the job description\n",
        "    # It asks for a structured analysis with 4 specific sections: requirements, matches, gaps, and strengths\n",
        "    prompt = f\"\"\"\n",
        "    Context:\n",
        "    You are a career advisor and resume expert. Your task is to analyze a candidate's resume against a specific job description to assess alignment and identify areas for improvement.\n",
        "\n",
        "    Instruction:\n",
        "    Review the provided Job Description and Resume. Identify key skills, experiences, and qualifications in the Job Description and compare them to what's present in the Resume. Provide a structured analysis with the following sections:\n",
        "    1. **Key Requirements from Job Description:** List the main skills, experiences, and qualifications sought by the employer.\n",
        "    2. **Relevant Experience in Resume:** List the skills and experiences from the resume that match or align closely with the job requirements.\n",
        "    3. **Gaps/Mismatches:** Identify important skills or qualifications from the Job Description that are missing, unclear, or underrepresented in the Resume.\n",
        "    4. **Potential Strengths:** Highlight any valuable skills, experiences, or accomplishments in the resume that are not explicitly requested in the job description but could strengthen the application.\n",
        "\n",
        "    Job Description:\n",
        "\n",
        "    {job_description_text}\n",
        "\n",
        "    Resume:\n",
        "\n",
        "    {resume_text}\n",
        "\n",
        "    Output:\n",
        "    Return a clear, structured comparison with the four sections outlined above.\n",
        "    \"\"\"\n",
        "\n",
        "    # This conditional block selects which AI model to use based on the 'model' parameter\n",
        "    if model == \"openai\":\n",
        "        # Uses OpenAI's model to generate the gap analysis with moderate creativity (temperature=0.7)\n",
        "        gap_analysis = openai_generate(prompt, temperature=0.7)\n",
        "    elif model == \"gemini\":\n",
        "        # Uses Google's Gemini model with less creativity (temperature=0.5) for more focused results\n",
        "        gap_analysis = gemini_generate(prompt, temperature=0.5)\n",
        "    else:\n",
        "        # Raises an error if an invalid model name is provided\n",
        "        raise ValueError(f\"Invalid model: {model}\")\n",
        "\n",
        "    # Returns the generated gap analysis text\n",
        "    return gap_analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "2IgArFsyggkO",
        "outputId": "555081cd-398a-4419-b9b7-ea16c62ce7ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### OpenAI Response:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### 1. Key Requirements from Job Description:\n- **Education**: A degree in Computer Science, Data Science, or a related field.\n- **Machine Learning Knowledge**: Good understanding of machine learning algorithms (regression, classification, clustering).\n- **Programming Skills**: Proficient in Python and libraries (such as NumPy, Pandas, Scikit-learn).\n- **Cloud Platforms**: Familiarity with cloud platforms like AWS, Google Cloud, or Azure.\n- **NLP or Computer Vision**: Knowledge of Natural Language Processing (NLP) or computer vision is a plus.\n- **Version Control**: Comfortable with version control systems, particularly Git.\n- **Model Deployment**: Experience in deploying and testing models in production-like environments.\n- **Collaboration**: Ability to collaborate with software engineers to integrate models.\n- **Record Keeping**: Keeping detailed records of processes, findings, models, and performance.\n\n### 2. Relevant Experience in Resume:\n- **Education**: Bachelor of Science in Computer Science (2021 – 2024) from University of Karachi.\n- **Machine Learning Knowledge**: Experience with ML algorithms including regression, classification, and clustering as demonstrated through projects.\n- **Programming Skills**: Proficient in Python and libraries such as NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch.\n- **NLP and Computer Vision**: Demonstrated knowledge in both fields through projects such as sentiment analysis and traffic sign detection.\n- **Model Deployment**: Experience deploying models using Flask and Gradio, as seen in various projects.\n- **Collaboration**: Worked as a Data Science Fellow where collaboration was a part of building solutions.\n- **Record Keeping**: Inferred experience with detailed documentation from project descriptions.\n\n### 3. Gaps/Mismatches:\n- **Cloud Platforms**: The resume does not mention any familiarity with cloud platforms like AWS, Google Cloud, or Azure, which is a requirement in the job description.\n- **Version Control**: There is no mention of experience with Git or any version control systems, which is explicitly requested.\n- **Specific Techniques**: While the resume mentions a range of machine learning techniques, it does not specifically highlight the exploration or testing of new machine learning techniques.\n- **Production Environment**: Although the resume mentions deployment, it lacks detail on experience in production-like environments as specified in the job description.\n\n### 4. Potential Strengths:\n- **Diverse Project Experience**: The resume showcases a variety of projects (e.g., medical diagnostics, sentiment analysis, real-time traffic sign detection) which demonstrate not only technical skills but also creativity and problem-solving abilities.\n- **Recognition**: The \"Fellow of the Month – June’24\" recognition at Bytewise Limited indicates strong performance and can enhance the candidate's profile.\n- **Soft Skills**: The candidate mentions being a fast learner, proactive team player, and growth-oriented, which are valuable traits for an intern role.\n- **Advanced Techniques**: Experience with cutting-edge technologies like LangChain and LLaMA3 in building contextual question-answering systems can be appealing to employers looking for innovation.\n\n### Conclusion:\nOverall, Syed Abdullah Hasan's resume aligns well with many of the job description's requirements, particularly in terms of education, programming skills, and machine learning knowledge. However, there are notable gaps regarding cloud platform experience and version control knowledge that should be addressed. Strengthening these areas and emphasizing collaborative and practical experiences can enhance the overall application."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Call the function to analyze the resume against the job description using OpenAI\n",
        "gap_analysis_openai = analyze_resume_against_job_description(job_description_text,\n",
        "                                                             resume_text,\n",
        "                                                             model = \"openai\")\n",
        "\n",
        "# Displays the analysis results in Markdown format\n",
        "print_markdown(\"#### OpenAI Response:\")\n",
        "print_markdown(gap_analysis_openai)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQtIZhvoggkP"
      },
      "source": [
        "# DRAFTING A NEW TAILORED RESUME BY AI WITH CHANGE TRACKING (WITH PYDANTIC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9mOnByhggkQ"
      },
      "source": [
        "Now, we'll use the insights gained (analysis and suggestions) to generate a completely rewritten, tailored resume using OpenAI. A key enhancement here is to ask the AI not just to list the changes, but to try and identify which sections or areas of the resume were modified. This helps the user quickly see the impact of the tailoring.\n",
        "\n",
        "We'll ask the AI for two outputs:\n",
        "1.  The full text of the tailored resume.\n",
        "2.  A structured list (using Markdown) describing the key changes and where they were made (e.g., Summary, Experience section, Skills)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "NoCbz91mggkQ"
      },
      "outputs": [],
      "source": [
        "# Define Pydantic models for structured output\n",
        "# The ResumeOutput class is a Pydantic model that defines the structure of the output\n",
        "# for the resume generation function. It includes two fields:\n",
        "# (1) updated_resume: A string that contains the final rewritten resume.\n",
        "# (2) diff_markdown: A string containing the resume's HTML-coloured version highlighting additions and deletions.\n",
        "\n",
        "class ResumeOutput(BaseModel):\n",
        "    updated_resume: str\n",
        "    diff_markdown: str\n",
        "\n",
        "\n",
        "def generate_resume(\n",
        "    job_description_text: str, resume_text: str, gap_analysis_openai: str, model: str = \"openai\") -> dict:\n",
        "    \"\"\"\n",
        "    Generate a tailored resume using OpenAI or Gemini.\n",
        "\n",
        "    Args:\n",
        "        job_description_text (str): The job description text.\n",
        "        resume_text (str): The candidate's resume text.\n",
        "        gap_analysis_openai (str): The gap analysis result from OpenAI.\n",
        "        model (str): The model to use for resume generation.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the updated resume and diff markdown.\n",
        "    \"\"\"\n",
        "    # Construct the prompt for the AI model to generate the tailored resume.\n",
        "    # The prompt includes context, instructions, and input data (original resume,\n",
        "    # target job description, and gap analysis).\n",
        "    prompt = (\n",
        "        \"\"\"\n",
        "    ### Context:\n",
        "    You are an expert resume writer and editor. Your goal is to rewrite the original resume to match the target job description, using the provided tailoring suggestions and analysis.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Instruction:\n",
        "    1. Rewrite the entire resume to best match the **Target Job Description** and **Gap Analysis to the Job Description**.\n",
        "    2. Improve clarity, add job-relevant keywords, and quantify achievements.\n",
        "    3. Specifically address the gaps identified in the analysis by:\n",
        "       - Adding missing skills and technologies mentioned in the job description\n",
        "       - Reframing experience to highlight relevant accomplishments\n",
        "       - Strengthening sections that were identified as weak in the analysis\n",
        "    4. Prioritize addressing the most critical gaps first\n",
        "    5. Incorporate industry-specific terminology from the job description\n",
        "    6. Ensure all quantifiable achievements are properly highlighted with metrics\n",
        "    7. Return two versions of the resume:\n",
        "        - `updated_resume`: The final rewritten resume (as plain text)\n",
        "        - `diff_html`: A version of the resume with inline highlights using color:\n",
        "            - Additions or rewritten content should be **green**:\n",
        "            `<span style=\"color:green\">your added or changed text</span>`\n",
        "            - Removed content should be **red and struck through**:\n",
        "            `<span style=\"color:red;text-decoration:line-through\">removed text</span>`\n",
        "            - Leave unchanged lines unmarked.\n",
        "        - Keep all section headers and formatting consistent with the original resume.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Output Format:\n",
        "\n",
        "    ```json\n",
        "    {\n",
        "    \"updated_resume\": \"<full rewritten resume as plain text>\",\n",
        "    \"diff_markdown\": \"<HTML-colored version of the resume highlighting additions and deletions>\"\n",
        "    }\n",
        "    ```\n",
        "    ---\n",
        "    ### Input:\n",
        "\n",
        "    **Original Resume:**\n",
        "\n",
        "    \"\"\"\n",
        "        + resume_text\n",
        "        + \"\"\"\n",
        "\n",
        "\n",
        "    **Target Job Description:**\n",
        "\n",
        "    \"\"\"\n",
        "        + job_description_text\n",
        "        + \"\"\"\n",
        "\n",
        "\n",
        "    **Analysis of Resume vs. Job Description:**\n",
        "\n",
        "    \"\"\"\n",
        "        + gap_analysis_openai\n",
        "    )\n",
        "\n",
        "    # Depending on the selected model, call the appropriate function to generate the resume.\n",
        "    # If the OpenAI model is selected, it uses a temperature of 0.7 for creativity.\n",
        "    if model == \"openai\":\n",
        "        updated_resume_json = openai_generate(prompt, temperature = 0.7, response_format = ResumeOutput)\n",
        "    # If the Gemini model is selected, it uses a lower temperature of 0.5 for more focused results.\n",
        "    elif model == \"gemini\":\n",
        "        updated_resume_json = gemini_generate(prompt, temperature = 0.5)\n",
        "    else:\n",
        "        # Raise an error if an invalid model name is provided.\n",
        "        raise ValueError(f\"Invalid model: {model}\")\n",
        "\n",
        "    # Return the generated resume output as a dictionary.\n",
        "    return updated_resume_json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KQgNlADaggkR",
        "outputId": "a49dc0f7-57fa-4ce8-e253-11fe311d3306"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Syed Abdullah Hasan  \nAI Engineer  \nKarachi, Pakistan | Ph: +923228220707 | abdullahhasan1045@gmail.com | LinkedIn: Abdullah Hasan  |  \nGithub: Abdullah Hasan  \n  \nSummary  \nMotivated AI Engineer with a robust foundation in machine learning, deep learning, computer vision, and natural language processing. Proficient in Python, TensorFlow, PyTorch, and modern AI frameworks. Hands-on experience in developing and deploying end-to-end AI solutions, including CNN-based medical diagnostics, LLM-powered QA systems, and NLP sentiment analysis. Eager to contribute to impactful projects and grow under mentorship in a collaborative environment while exploring new AI tools and methodologies.  \n  \nProfessional Experience   \nData Science Fellow – Bytewise Limited  \nJune 2024 – Sep 2024  \n• Developed and optimized ML models (including CNNs) for real-world data challenges; enhanced data cleaning, scaling, and encoding pipelines.  \n• Collaborated with software engineers to integrate innovative machine learning solutions into existing systems, ensuring seamless deployment in production-like environments.  \n• Committed to advancing machine learning skills through practical experience, ongoing education, and exploration of new techniques.  \n• Fellow of the Month – June’24, Bytewise Limited  \n  \nAcademic Qualification   \nUniversity Of Karachi - UBIT  \n2021 – 2024  \n• Bachelor of Science in Computer Science  \n  \nCourse Certifications:  \n• Certified Associate Data Scientist by DataCamp  \n• Data Manipulation with pandas by DataCamp  \n• Exploratory Data Analysis in Python  \n• Machine Learning from Udemy  \n• Data Science by Plus W 株式会社  \n  \nSkills:  \nLanguages & Libraries : Python, NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch, LangChain  \nML & AI Development : Regression, Classification, Clustering, Transfer Learning, CNNs, Attention Mechanisms, LLMs  \nData Handling : Data cleaning, feature engineering, SQL, NoSQL  \nCloud Platforms: Familiarity with AWS and Google Cloud  \nVersion Control: Git  \nVisualization : Power BI, Matplotlib, Seaborn  \nAI Deployment : Flask, Streamlit, Gradio  \nSoft Skills : Fast learner, proactive team player, growth-oriented mindset  \n  \nProjects:  \nBrain Hemorrhage Detection Using CNN (EfficientNetB2)  \nFinal Year Project  \n• Developed an end-to-end deep learning pipeline using EfficientNetB2 for multiclass classification of 5 brain hemorrhage types.  \n• Leveraged the RSNA_19 dataset and achieved 90%+ validation accuracy through advanced data augmentation and transfer learning.  \n• Preprocessed medical DICOM images using windowing techniques and merged multi-channel inputs for model readiness.  \n  \nStudent Performance Prediction System  \n• Built a regression model to predict students' math scores using demographic and academic features.  \n• Designed a modular Train-Predict pipeline for scalability, using one-hot encoding and feature standardization.  \n• Achieved an R² score of 88.76 using an SGD Regressor and deployed a lightweight Flask API for real-time predictions.  \n  \nSentiment Analysis for Amazon Product Reviews  \n• Created a sentiment classifier using a fine-tuned DistilRoBERTa model, reaching 94.8% accuracy.  \n• Preprocessed textual data using NLP techniques (tokenization, lemmatization, stopword removal).  \n• Integrated a FalconsAI summarization model and built a Gradio UI for real-time sentiment analysis and review summarization.  \n  \nLLM-Powered Contextual Question Answering System  \n• Developed a document-agnostic QA assistant using LangChain, Ollama, and LLaMA3 with RAG architecture.  \n• Implemented vector-based context retrieval, dynamic prompt templates, and fallback handling for unmatched queries.  \n• Enabled users to upload unstructured documents and receive accurate answers based on context.  \n  \nReal-Time Traffic Sign Detection with YOLOv8  \n• Built a computer vision system using YOLOv8 for real-time detection and classification of traffic signs.  \n• Trained the model on a custom-labeled dataset, achieving high mAP scores.  \n• Deployed with Gradio on Hugging Face Spaces for real-time browser inference.  \n  \nContent-Based Anime Recommendation System  \n• Developed a recommendation engine using TF-IDF vectorization and cosine similarity to suggest similar anime titles.  \n• Cleaned and vectorized textual metadata (genres, synopsis) for better recommendation quality.  \n• Packaged into an interactive application for user input and personalized suggestions."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Call the generate_resume function with the provided job description, resume text, and gap analysis.\n",
        "updated_resume_json = generate_resume(job_description_text, resume_text, gap_analysis_openai, model=\"openai\")\n",
        "# Display the updated resume in Markdown format.\n",
        "print_markdown(updated_resume_json.updated_resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ikik2MlMggkS",
        "outputId": "dfcd796b-4205-4e38-f56e-44eea6811e40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Syed Abdullah Hasan  <br>\nAI Engineer  <br>\nKarachi, Pakistan | Ph: +923228220707 | abdullahhasan1045@gmail.com | LinkedIn: Abdullah Hasan  |  <br>\nGithub: Abdullah Hasan  <br>\n  <br>\nSummary  <br>\nMotivated AI Engineer with a robust foundation in machine learning, deep learning, computer vision, and  <br>\nnatural language processing. Proficient in Python, TensorFlow, PyTorch, and modern AI frameworks. Hands-on  <br>\nexperience in developing and deploying end-to-end AI solutions, including CNN-based medical diagnostics,  <br>\nLLM-powered QA systems, and NLP sentiment analysis. Eager to contribute to impactful projects and grow under  <br>\nmentorship in a collaborative environment while exploring <span style=\"color:green\">new AI tools and methodologies.</span>  <br>\n  <br>\nProfessional Experience   <br>\nData Science Fellow – Bytewise Limited  <br>\nJune 2024 – Sep 2024  <br>\n• <span style=\"color:green\">Developed and optimized ML models</span> (including CNNs) for real-world data challenges; <span style=\"color:green\">enhanced</span> data cleaning, scaling, and encoding pipelines.  <br>\n• Collaborated with <span style=\"color:green\">software engineers to integrate innovative machine learning solutions into existing systems, ensuring seamless deployment in production-like environments.</span>  <br>\n• Committed to advancing machine learning skills through practical experience, ongoing education, and <span style=\"color:green\">exploration of new techniques.</span>  <br>\n• Fellow of the Month – June’24, Bytewise Limited  <br>\n  <br>\nAcademic Qualification   <br>\nUniversity Of Karachi - UBIT  <br>\n2021 – 2024  <br>\n• Bachelor of Science in Computer Science  <br>\n  <br>\nCourse Certifications:  <br>\n• Certified Associate Data Scientist by DataCamp  <br>\n• Data Manipulation with pandas by DataCamp  <br>\n• Exploratory Data Analysis in Python  <br>\n• Machine Learning from Udemy  <br>\n• Data Science by Plus W 株式会社  <br>\n  <br>\nSkills:  <br>\nLanguages & Libraries : Python, NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch, LangChain  <br>\nML & AI Development : Regression, Classification, Clustering, Transfer Learning, CNNs, Attention Mechanisms, LLMs  <br>\nData Handling : Data cleaning, feature engineering, SQL, NoSQL  <br>\n<span style=\"color:green\">Cloud Platforms: Familiarity with AWS and Google Cloud</span>  <br>\n<span style=\"color:green\">Version Control: Git</span>  <br>\nVisualization : Power BI, Matplotlib, Seaborn  <br>\nAI Deployment : Flask, Streamlit, Gradio  <br>\nSoft Skills : Fast learner, proactive team player, growth-oriented mindset  <br>\n  <br>\nProjects:  <br>\nBrain Hemorrhage Detection Using CNN (EfficientNetB2)  <br>\nFinal Year Project  <br>\n• Developed an end-to-end deep learning pipeline using EfficientNetB2 for multiclass classification of 5 brain hemorrhage types.  <br>\n• Leveraged the RSNA_19 dataset and achieved 90%+ validation accuracy through advanced data augmentation and transfer learning.  <br>\n• Preprocessed medical DICOM images using windowing techniques and merged multi-channel inputs for model readiness.  <br>\n  <br>\nStudent Performance Prediction System  <br>\n• Built a regression model to predict students' math scores using demographic and academic features.  <br>\n• Designed a modular Train-Predict pipeline for scalability, using one-hot encoding and feature standardization.  <br>\n• Achieved an R² score of 88.76 using an SGD Regressor and deployed a lightweight Flask API for real-time predictions.  <br>\n  <br>\nSentiment Analysis for Amazon Product Reviews  <br>\n• Created a sentiment classifier using a fine-tuned DistilRoBERTa model, reaching 94.8% accuracy.  <br>\n• Preprocessed textual data using NLP techniques (tokenization, lemmatization, stopword removal).  <br>\n• Integrated a FalconsAI summarization model and built a Gradio UI for real-time sentiment analysis and review summarization.  <br>\n  <br>\nLLM-Powered Contextual Question Answering System  <br>\n• Developed a document-agnostic QA assistant using LangChain, Ollama, and LLaMA3 with RAG architecture.  <br>\n• Implemented vector-based context retrieval, dynamic prompt templates, and fallback handling for unmatched queries.  <br>\n• Enabled users to upload unstructured documents and receive accurate answers based on context.  <br>\n  <br>\nReal-Time Traffic Sign Detection with YOLOv8  <br>\n• Built a computer vision system using YOLOv8 for real-time detection and classification of traffic signs.  <br>\n• Trained the model on a custom-labeled dataset, achieving high mAP scores.  <br>\n• Deployed with Gradio on Hugging Face Spaces for real-time browser inference.  <br>\n  <br>\nContent-Based Anime Recommendation System  <br>\n• Developed a recommendation engine using TF-IDF vectorization and cosine similarity to suggest similar anime titles.  <br>\n• Cleaned and vectorized textual metadata (genres, synopsis) for better recommendation quality.  <br>\n• Packaged into an interactive application for user input and personalized suggestions.  <br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print_markdown(updated_resume_json.diff_markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLTWfmniggkS"
      },
      "source": [
        "# GENERATING A CUSTOM COVER LETTER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32SA7ha7ggkT"
      },
      "source": [
        "With the newly tailored resume, let's generate a corresponding cover letter.\n",
        "We'll use OpenAI, feeding it the tailored resume (from the previous task) and the original job description. This ensures the cover letter highlights the most relevant points from the improved resume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XNfF8XDBggkT"
      },
      "outputs": [],
      "source": [
        "# Define Pydantic models for structured output\n",
        "# The CoverLetterOutput class is a Pydantic model that defines the structure of the output for the cover letter generation.\n",
        "# It ensures that the output will contain a single field, 'cover_letter', which is a string.\n",
        "\n",
        "class CoverLetterOutput(BaseModel):\n",
        "    cover_letter: str\n",
        "\n",
        "# The generate_cover_letter function creates a cover letter based on the provided job description and updated resume.\n",
        "# It takes three parameters:\n",
        "# (1) job_description_text: A string containing the job description for the position.\n",
        "# (2) updated_resume: A string containing the candidate's updated resume.\n",
        "# (3) model: A string indicating which model to use for generating the cover letter (default is \"openai\").\n",
        "# The function returns a dictionary containing the generated cover letter.\n",
        "\n",
        "def generate_cover_letter(job_description_text: str, updated_resume: str, model: str = \"openai\") -> dict:\n",
        "    \"\"\"\n",
        "    Generate a cover letter using OpenAI or Gemini.\n",
        "\n",
        "    Args:\n",
        "        job_description_text (str): The job description text.\n",
        "        updated_resume (str): The candidate's updated resume text.\n",
        "        model (str): The model to use for cover letter generation.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the cover letter.\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct the prompt for the AI model, including context and instructions for writing the cover letter.\n",
        "    prompt = (\n",
        "        \"\"\"\n",
        "    ### Context:\n",
        "    You are a professional career coach and expert cover letter writer.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Instruction:\n",
        "    Write a compelling, personalized cover letter based on the **Updated Resume** and the **Target Job Description**. The letter should:\n",
        "    1. Be addressed generically (e.g., \"Dear Hiring Manager\")\n",
        "    2. Be no longer than 4 paragraphs\n",
        "    3. Highlight key achievements and experiences from the updated resume\n",
        "    4. Align with the responsibilities and qualifications in the job description\n",
        "    5. Reflect the applicant's enthusiasm and fit for the role\n",
        "    6. End with a confident and polite closing statement\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Output Format (JSON):\n",
        "    ```json\n",
        "    {\n",
        "    \"cover_letter\": \"<final cover letter text>\"\n",
        "    }\n",
        "    ```\n",
        "    ---\n",
        "\n",
        "    ### Input:\n",
        "\n",
        "    **Updated Resume:**\n",
        "\n",
        "    \"\"\"\n",
        "        + updated_resume\n",
        "        + \"\"\"\n",
        "    **Target Job Description:**\n",
        "\n",
        "    \"\"\"\n",
        "        + job_description_text\n",
        "    )\n",
        "\n",
        "    # Depending on the selected model, call the appropriate function to generate the cover letter.\n",
        "    if model == \"openai\":\n",
        "        # Get response from OpenAI API\n",
        "        updated_cover_letter = openai_generate(prompt, temperature=0.7, response_format=CoverLetterOutput)\n",
        "    elif model == \"gemini\":\n",
        "        # Get response from Gemini API\n",
        "        updated_cover_letter = gemini_generate(prompt, temperature=0.5)\n",
        "    else:\n",
        "        # Raise an error if an invalid model name is provided.\n",
        "        raise ValueError(f\"Invalid model: {model}\")\n",
        "\n",
        "    # Return the generated cover letter as a dictionary.\n",
        "    return updated_cover_letter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "2lPLjee9ggkV",
        "outputId": "790982d9-9a9f-401c-874d-1b0250bec034"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Dear Hiring Manager,\n\nI am writing to express my enthusiasm for the AI/ML Intern position at your esteemed organization. With a Bachelor of Science in Computer Science from the University of Karachi and hands-on experience in machine learning and AI, I am eager to contribute my skills to your team and work on impactful projects. My recent role as a Data Science Fellow at Bytewise Limited allowed me to develop and optimize machine learning models, including CNNs for real-world data challenges, which aligns perfectly with the responsibilities of this internship.\n\nThroughout my experience, I have successfully collaborated with software engineers to integrate innovative machine learning solutions into existing systems, ensuring seamless deployments. For instance, I built a real-time traffic sign detection system utilizing YOLOv8 and deployed it using Gradio, showcasing my proficiency in model deployment and my ability to work in production-like environments. Additionally, my academic projects, such as the LLM-Powered Contextual Question Answering System, demonstrate my skills in NLP and my commitment to exploring new AI techniques to improve model accuracy.\n\nI am particularly excited about the opportunity to work in a supportive environment that emphasizes mentorship and professional development. Coupled with my proactive nature and growth-oriented mindset, I am confident in my ability to contribute positively to your team while continuing to enhance my skills in machine learning and AI.\n\nThank you for considering my application. I look forward to the possibility of discussing how my background, skills, and enthusiasm can contribute to the innovative projects at your company.\n\nSincerely,\nSyed Abdullah Hasan"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Call the generate_cover_letter function with the provided job description and updated resume.\n",
        "updated_cover_letter = generate_cover_letter(job_description_text, updated_resume_json.updated_resume, model=\"openai\")\n",
        "\n",
        "# Display the generated cover letter in Markdown format.\n",
        "print_markdown(updated_cover_letter.cover_letter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXsEWk4gggkX"
      },
      "source": [
        "# UNIFIENING RESUME AND COVER LETTER GENERATION FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeAoD19PggkY"
      },
      "source": [
        "Now that we have all the building blocks, let's create a single function, `run_resume_rocket`, that takes the original resume and job description text and performs the entire workflow: gap analysis, resume tailoring with diff tracking, and cover letter generation. This makes the tool much easier to reuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "EhhZd3svggkZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def run_resume_rocket(resume, job_description_text: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Run the resume rocket workflow.\n",
        "\n",
        "    Args:\n",
        "        resume_text (str): The candidate's resume text.\n",
        "        job_description_text (str): The job description text.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated resume and cover letter.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Validation: Both inputs required ---\n",
        "    if resume is None:\n",
        "        return \"⚠️ Please upload a resume file.\", \"\"\n",
        "    if not job_description_text.strip():\n",
        "        return \"\", \"⚠️ Please paste the job description.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Analyze the candidate's resume against the job description using OpenAI's model.\n",
        "    # This function will return a structured analysis highlighting gaps and strengths.\n",
        "\n",
        "\n",
        "\n",
        "     # Detect file type\n",
        "    file_path = resume.name\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        resume_text = extract_text_from_pdf(file_path)\n",
        "    else:\n",
        "        # fallback for txt or unknown\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            resume_text = f.read()\n",
        "\n",
        "\n",
        "    gap_analysis_openai = analyze_resume_against_job_description(job_description_text,\n",
        "                                                                 resume_text,\n",
        "                                                                 model=\"openai\")\n",
        "\n",
        "    # Display the gap analysis results in Markdown format for better readability.\n",
        "    print_markdown(gap_analysis_openai)\n",
        "\n",
        "    # Print separators for clarity in the output.\n",
        "    print(\"\\n--------------------------------\")\n",
        "    print(\"--------------------------------\\n\")\n",
        "\n",
        "    # Generate an updated resume based on the job description, original resume, and gap analysis.\n",
        "    # This function will return a JSON-like object containing the updated resume and a diff markdown.\n",
        "    updated_resume_json = generate_resume(job_description_text,\n",
        "                                          resume_text,\n",
        "                                          gap_analysis_openai,\n",
        "                                          model = \"openai\")\n",
        "\n",
        "    # Display the diff markdown which shows the changes made to the resume.\n",
        "    print_markdown(updated_resume_json.diff_markdown)\n",
        "\n",
        "    # Print separators for clarity in the output.\n",
        "    print(\"\\n--------------------------------\")\n",
        "    print(\"--------------------------------\\n\")\n",
        "\n",
        "    # Display the updated resume in Markdown format.\n",
        "    print_markdown(updated_resume_json.updated_resume)\n",
        "\n",
        "    # Print separators for clarity in the output.\n",
        "    print(\"\\n--------------------------------\")\n",
        "    print(\"--------------------------------\\n\")\n",
        "\n",
        "    # Generate a cover letter based on the job description and the updated resume.\n",
        "    # This function will return the generated cover letter.\n",
        "    updated_cover_letter = generate_cover_letter(\n",
        "        job_description_text, updated_resume_json.updated_resume, model=\"openai\"\n",
        "    )\n",
        "\n",
        "    # Display the generated cover letter in Markdown format.\n",
        "    print_markdown(updated_cover_letter.cover_letter)\n",
        "\n",
        "    # Print separators for clarity in the output.\n",
        "    print(\"\\n--------------------------------\")\n",
        "    print(\"--------------------------------\\n\")\n",
        "\n",
        "    # Return the updated resume and the generated cover letter as a tuple.\n",
        "    return updated_resume_json.updated_resume, updated_cover_letter.cover_letter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "QBxLK-RJggka"
      },
      "outputs": [],
      "source": [
        "# # Call the run_resume_rocket function with the provided resume and job description texts.\n",
        "# resume, cover_letter = run_resume_rocket(resume_text, job_description_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Gradio App"
      ],
      "metadata": {
        "id": "qqlsotUOq4wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Ocean()) as demo:\n",
        "    gr.Markdown(\"# Resume Maker\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### Upload Resume & Job Description\\nGet an **Updated Resume** and a **Personalized Cover Letter**\")\n",
        "\n",
        "            resume_upload = gr.File(label=\"Upload Resume\", file_types=[\".pdf\", \".docx\", \".txt\"])\n",
        "            job_desc = gr.TextArea(label=\"Paste Job Description Here\", lines=5, placeholder=\"Paste the job description...\")\n",
        "\n",
        "            generate_btn = gr.Button(\"Generate\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            updated_resume = gr.Textbox(label=\"Updated Resume\", lines=15, interactive=False)\n",
        "            cover_letter = gr.Textbox(label=\"Cover Letter\", lines=15, interactive=False)\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=run_resume_rocket,\n",
        "        inputs=[resume_upload, job_desc],\n",
        "        outputs=[updated_resume, cover_letter]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "er2Rjy3nq6hl",
        "outputId": "c14ebb8d-d1ad-4399-d6c4-121590dc534f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cb4fb815a63d217f1f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cb4fb815a63d217f1f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}